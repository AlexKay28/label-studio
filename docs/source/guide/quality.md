---
title: Review annotations in Label Studio
type: guide
order: 404
meta_title: 
meta_description:
---

After multiple labelers have annotated tasks, review their output to validate the quality of the results. You can also perform this task after a model has predicted labels for tasks in your dataset. 

> The annotation review workflow is only available in Label Studio Enterprise Edition. If you're using Label Studio Community Edition, CONTACT US to learn more about the Enterprise Edition. 

## Why review annotations?

Data labeling is a crucial step for training many machine learning models, and it's essential to review annotations to make sure that only the highest quality data is used to train your machine learning model. If you don't review the quality of labeled data, weak annotations might be used when training your model and degrade overall model performance. READ MORE IN THIS BLOG POST???

## Choosing what to review

You can start reviewing tasks randomly, or order tasks in the project data manager in different ways, depending on your use case:
- Order tasks by annotator, to review annotations and assess individual annotator performance at the same time.
- Order tasks by consensus score, to review annotations with more uncertainty first. 
- Order tasks by model confidence score, to review the annotations that a machine learning model was less certain about first. 

## Review annotated tasks

After you choose what to review, from inside a project click the blue **Review** button. 




## ? Automatically identify mislabeled results from machine learning models

## Verify model and annotation performance (?)

### Review annotations against ground truth labels (?)

### Review annotator consensus 
